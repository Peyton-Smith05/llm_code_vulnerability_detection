# LLaMA2 

* Repository: [LLaMA2](https://github.com/facebookresearch/llama)

* Paper: [LLaMA2 Open Foundation and Fine Tuned Chat Models](https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/)

* HuggingFace Repo: [LLaMA2 HuggingFace](https://huggingface.co/meta-llama)

* More info: [LLaMA2 article](https://huggingface.co/docs/transformers/model_doc/llama2)

LLaMA 2 is Meta's Large Language Model. The reason I chose to work with this model the most was its ability to be downloaded and queried locally, as well as the ability to finetune it. It seems to be one of the best performing models with these abilities. 

## Installing LLaMA 2

* Request to download LLaMA 2 from the [download link](https://ai.meta.com/resources/models-and-libraries/llama-downloads/). They will grant access within 24 hours.

* Once you are granted access, clone the [Github Repo](https://github.com/facebookresearch/llama). In the repo there is instructions on how to install the models. If you are doing this on the bigmlserver with 2 NVIDIA A10's the largest model that can be used is the 13b parameter model. 

* Once installed I changed their `example_chat_completion.py` script to use data from the juliet dataset and ask it to classify CWE errors in code. It then compares it with the actual CWE error and gets the accuracy for classification with out any finetuning. If you replace the code in this repository with the one in meta's repository and run the below command you will be querying the model for bug classification.

```unix
torchrun --nproc_per_node 1 example_text_completion.py \
    --ckpt_dir llama-2-13b/ \
    --tokenizer_path tokenizer.model \
    --max_seq_len 2048 --max_batch_size 4
```