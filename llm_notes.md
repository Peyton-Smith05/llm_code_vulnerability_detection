## Notes

### Model types:

Hugging Face has two main types of llm models on it. Text-Generation and Text2Text-Generation.

* Text Generation: Causal and good for adapting to follow instructions. 
    * Use Cases: Code Generation, Stories Generation
    * It basically is a very fancy autofill system that predicts the next word or line given a an input token. StarCoder is a coder generation autocomplete system. 
* Text2Text-Generation is a Variant of Text-Generation which is non-causal. It takes in multiple words/sentence or task and produces multiple variants of possible outputs. 


### Other

* Inference: the act or ability of an LLM to look at words and gain context which then allows it to infer what comes next. 

* Temperature: Controls the randomness of the output. The higher the temperature the more random/creative the output is. 


## Errors and Resolution Documentation

### GPU out of Memory
```
OutOfMemoryError                          Traceback (most recent call last)
Cell In[14], line 5
      1 # model_kwargs = {"device_map": "auto", "load_in_8bit": True}
      2 # m = AutoModelForCausalLM.from_pretrained(model_id, device_map="auto")
      3 # tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=False)
      4 # generator = pipeline(task="text-generation", model=m, tokenizer=tokenizer, device=0, model_kwargs=model_kwargs)
----> 5 llm = HuggingFacePipeline.from_model_id(
      6     model_id=model_id,
      7     task="text-generation",
      8     model_kwargs={"temperature": 0, "max_length": 1000},
      9     device=1,
     10 )

... 

File ~/llm/env/lib/python3.10/site-packages/torch/nn/modules/module.py:1143, in Module.to.<locals>.convert(t)
   1140 if convert_to_format is not None and t.dim() in (4, 5):
   1141     return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,
   1142                 non_blocking, memory_format=convert_to_format)
-> 1143 return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)

OutOfMemoryError: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 1; 23.69 GiB total capacity; 23.35 GiB already allocated; 41.69 MiB free; 23.35 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
```

#### Resolution

* The Model was 31 gb and the nvidia drives only have 24gb of ram. I tried seeing if there was a way to run it on both nvidia drives but did not see much with the current package I am using. The next solution is to find another smaller model.


## Llama Code

* Installed llama code following [github repo](https://github.com/facebookresearch/codellama) instructions.

* I installed the 7b and 13b parameter model.

* run: 
```
torchrun --nproc_per_node 1 example_completion.py \
    --ckpt_dir CodeLlama-7b/ \
    --tokenizer_path CodeLlama-7b/tokenizer.model \
    --max_seq_len 128 --max_batch_size 4
```

* The example_completion .py has an example of how to use the llama api in python to query the model. Next task to look how to use it to finetune a model.

* the followings for infilling which the code-llama can do. Note since I installed both I can run both 13b and 7b
```
torchrun --nproc_per_node 1 example_infilling.py \
    --ckpt_dir CodeLlama-13b/ \
    --tokenizer_path CodeLlama-7b/tokenizer.model \
    --max_seq_len 192 --max_batch_size 4
```